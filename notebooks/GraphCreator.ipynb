{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-06T18:12:21.897645Z",
     "start_time": "2019-09-06T18:12:21.130956Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../utils/\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler, MaxAbsScaler, RobustScaler\n",
    "\n",
    "from GraphAPI import GraphCreator\n",
    "from graph_helpers import *\n",
    "from evaluations import *\n",
    "from visualizers import *\n",
    "\n",
    "%aimport GraphAPI\n",
    "%aimport graph_helpers\n",
    "%aimport evaluations\n",
    "%aimport visualizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Graph from Entry Point\n",
    "\n",
    "1. We initialize our GraphCreator class and check how many new nodes we will need to query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-06T18:12:36.185Z"
    }
   },
   "outputs": [],
   "source": [
    "# include_see_also=False used for validation below.\n",
    "# in deployment, include_see_also should be set to True\n",
    "gc = GraphCreator(\"Sonata form\",\n",
    "                  include_see_also=False, max_recursive_requests=50)\n",
    "print(\"Number of Links to Search:\", len(gc.next_links), \"\\n\\n\")\n",
    "print(list(gc.primary_nodes.keys()), \"\\n\\n\")\n",
    "print(gc.see_also_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We query all the nodes linked to/from the entry point (expand our network one level for each node)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-06T18:06:40.627Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gc.expand_network_threaded(threads=20, chunk_size=1)\n",
    "print(\"Number of Links After Expansion: \", len(gc.graph.nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Since some nodes will likely have linked to articles through a redirect link, we need to traverse our graph and ensure that all redirects are assigned to the correct nodes. Once all redirects have been dealt with, we remove any old redirect nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-06T18:06:40.936Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.redraw_redirects()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Edges are weighted by how many categories two connected nodes have in common. Once we have all our nodes, and we have dealt with redirects, we can add edge weights for our entire graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-06T18:06:41.213Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.update_edge_weights()\n",
    "gc.get_edge_weights().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Our Feature Set\n",
    "\n",
    "There are two options when generating the feature set:\n",
    "\n",
    "1. we can generate a standard feature set with only the features themselves. To do this, have the `rank` parameter set to `False`.\n",
    "2. We can generate a ranked feature set (set `rank` equal to `True`). For each parameter, this will rank them in order of _best_ to _worst_ (this could be ascending or descending, depending on the context of the feature).\n",
    "\n",
    "After running `get_features_df`, the feature set will be saved in the GraphCreator instance as `feature_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-06T18:06:41.508Z"
    }
   },
   "outputs": [],
   "source": [
    "features_df = gc.get_features_df(rank=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-06T18:06:41.653Z"
    }
   },
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Rank\n",
    "\n",
    "Two articles are more similar the more categories they share and the closer they are to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-06T18:06:41.963Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gc.rank_similarity()\n",
    "gc.features_df.sort_values(\"similarity_rank\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Features\n",
    "\n",
    "We can easily scale our each of our features through the `scale_features_df` method. It will default to `Standard Scaler`, but we can specify alternate scalers in the `scaler` parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-06T18:06:42.268Z"
    }
   },
   "outputs": [],
   "source": [
    "scaled_feature_df = gc.scale_features_df(scaler=Normalizer, copy=True) # Makes a copy of the df\n",
    "scaled_feature_df.sort_values(\"similarity_rank\", ascending=False).reset_index().drop(\"index\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Validation\n",
    "\n",
    "Here, we _validate_ our results. For many articles, we already have some user defined links that are highly related to the present article. These are found in the **See Also** section of several Wikipedia articles (some pages do not have them). These are not ordered in importance in any meaningful way, and there are no rating scores.\n",
    "\n",
    "The intuition in this validation is as follows: \n",
    "\n",
    "> _Given that we know some articles are highly related from user input, if the recommendations provided by this system are valid, we would expect to see those **See Also** links ranked relatively high on our list._ \n",
    "\n",
    "_Note: This validation is not meant as **confirmation** or **evaluation** of the results. It only provides us one way of telling if the results we are seeing are reasonably valid. It is important to note that we cannot compare these results across two different articles, as those would be two entirely different network structures, likely with different human labeled links._  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-06T18:06:42.980Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_df = evaluate_metrics(scaled_feature_df, \n",
    "                 on=[\n",
    "                     \"similarity_rank\", \n",
    "                     \"degree\",\n",
    "                     \"category_matches_with_source\",\n",
    "                     \"in_edges\",\n",
    "                     \"out_edges\",\n",
    "                     \"shared_neighbors_with_entry_score\",\n",
    "                     \"centrality\", \n",
    "                     \"adjusted_reciprocity\", \n",
    "                     \"page_rank\", \n",
    "                     \"shortest_path_length_from_entry\", \n",
    "                     \"shortest_path_length_to_entry\",\n",
    "                     \"jaccard_similarity\"\n",
    "                 ], \n",
    "                 targets=gc.see_also_articles).sort_values([\"% targets in top 1%\", 'score'], ascending=False)\n",
    "validation_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chart generated above compares different ranking metrics (left index) for a given article. The most important column, `score`, provides a fast way for us to compare these different metrics. \n",
    "\n",
    "For example, if we see a _score_ of 0.98 for a given ranking metric, The following statement would be true:\n",
    "\n",
    "> All of the human labeled **See Also** links are present within the top 98% of our recommendations. \n",
    "\n",
    "Since the human labeled links comprise a range, it is not possible to get a score of 100%. The `max score possible` column indicates the score that would be achieved if all the human labeled _See Also_ links appeared at the top of our recommendations without any other links intervening.   \n",
    "\n",
    "The `difference` column is an alternative way of looking at the score. If we had a 0.02 in this column, we could say:\n",
    "\n",
    "> All the human labeled **See Also** links are contained within the top 2% of our recommendations. \n",
    "\n",
    "`Total targets` is the number of human labeled _See Also_ links. \n",
    "\n",
    "Because it is possible that different metrics could have similar scores, we want a way to break down the dispersion of the known related links to see if one metric does perform better than another. The trailing four columns provide us with a course way of measuring this dispersion. \n",
    "\n",
    "Each of these columns indicates the percentage of human labeled _See Also_ links captured within a given percentage of the top of our recommendations. For example, if we see a 0.92 in the `% targets in the top 1%` columns, we could say:\n",
    "\n",
    "> 92% of the human labeled **See Also** links appear in the top 1% of our recommendations. \n",
    "\n",
    "The value of these columns is a follows - If two ranking metrics have similar scores, we _might_ consider the better performing one to be the one in which the majority of the human labeled links are higher in our recommendation list. \n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-06T18:06:44.607Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_validation_scores(validation_df, gc.entry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
